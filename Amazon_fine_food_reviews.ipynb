{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Fine Food Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data Source: https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "* All data in one sqlite database. 568,454 food reviews Amazon users left up to October 2012\n",
    "* Total Columns:10\n",
    "* Columns List:Id,ProductId,UserId,ProfileName,HelpfulnessNumerator,HelpfulnessDenomenator,Score or Rating,Time,Summary,Text.\n",
    "* We are Droping Id column and changing our Score variable to Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing all necessary Libraries.\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from nltk.stem.porter import PorterStemmer  #Natural languge Tool Kit(nltk).\n",
    "\n",
    "#Reading data from SQLite Table\n",
    "con = sqlite3.connect(\"database.sqlite\")\n",
    "\n",
    "#Filtering Positive and Negative Reviews only Reviews having the score is equal to 3 are not considered.\n",
    "filtered_data = pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3\n",
    "\"\"\", con)\n",
    "\n",
    "#Assigning Positive rating to reviews having the score >3 otherwise negative rating\n",
    "def partition(x):\n",
    "    if x<3:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "#Change Reviews to Positive and Negative\n",
    "actual_score = filtered_data['Score']\n",
    "positiveNegative = actual_score.map(partition)\n",
    "filtered_data['Score'] = positiveNegative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525814, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting Shape of the Data Set atlast and Preview Data Set i.e some rows with all features or Variables\n",
    "print(filtered_data.shape)\n",
    "filtered_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138683</th>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417839</th>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId         UserId         ProfileName  \\\n",
       "138706  150524  0006641040  ACITT7DI6IDDL     shari zychinski   \n",
       "138683  150501  0006641040  AJ46FKXOVC7NR  Nicholas A Mesiano   \n",
       "417839  451856  B00004CXX9  AIUWLEQ1ADEG5    Elizabeth Medina   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator     Score       Time  \\\n",
       "138706                     0                       0  positive  939340800   \n",
       "138683                     2                       2  positive  940809600   \n",
       "417839                     0                       0  positive  944092800   \n",
       "\n",
       "                                                  Summary  \\\n",
       "138706                          EVERY book is educational   \n",
       "138683  This whole series is great way to spend time w...   \n",
       "417839                               Entertainingl Funny!   \n",
       "\n",
       "                                                     Text  \n",
       "138706  this witty little book makes my son laugh at l...  \n",
       "138683  I can remember seeing the show when it aired o...  \n",
       "417839  Beetlejuice is a well written movie ..... ever...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sorting the data according to ProductId in ascending order on filtered data\n",
    "sorted_data = filtered_data.sort_values('Time', axis=0, ascending=True)\n",
    "sorted_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning: Deduplication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364173, 10)\n"
     ]
    }
   ],
   "source": [
    "final = sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep='first',inplace=False)\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6925890143662968\n"
     ]
    }
   ],
   "source": [
    "#Finding How much percentage of the DATA Remaining.\n",
    "print((final['Id'].size)/(filtered_data['Id'].size))\n",
    "#69.25% of the data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing the data points that are having helpfulnessNum >= HelpfulnessDen (always den greater)\n",
    "final = final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n",
    "#final[\"HelpfulnessNumerator\"]<=final[\"HelpfulnessDenominator\"] results the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 10)\n"
     ]
    }
   ],
   "source": [
    "print(final.shape) #NOW Calculating the Rows and Colums available after removing above case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    307061\n",
       "negative     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442836</th>\n",
       "      <td>478823</td>\n",
       "      <td>B001QZYFOU</td>\n",
       "      <td>AEPG7I28BZKZB</td>\n",
       "      <td>Silcat3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1346803200</td>\n",
       "      <td>Yum, but mash it</td>\n",
       "      <td>Our three love the meat, and lick the plate if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226042</th>\n",
       "      <td>245088</td>\n",
       "      <td>B000GFYRHG</td>\n",
       "      <td>A2K3J2X8KDY47N</td>\n",
       "      <td>Jewelry Lover \"me\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1306972800</td>\n",
       "      <td>My all time favorite tea....</td>\n",
       "      <td>I am now receiving a case of this tea every si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232542</th>\n",
       "      <td>252248</td>\n",
       "      <td>B0046GSTUM</td>\n",
       "      <td>A2RUR1SMPNGKXJ</td>\n",
       "      <td>Katie</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>1345507200</td>\n",
       "      <td>Excellent very delicious</td>\n",
       "      <td>I am on a strict diet and this is a very delic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516810</th>\n",
       "      <td>558761</td>\n",
       "      <td>B000BVY02M</td>\n",
       "      <td>AOEDWQLH2WKKW</td>\n",
       "      <td>E. J Tastad \"ejt\"</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>positive</td>\n",
       "      <td>1169424000</td>\n",
       "      <td>Hot sauce concentrate, use at your own risk</td>\n",
       "      <td>This is like hot sauce concentrate.  You MUST ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141203</th>\n",
       "      <td>153228</td>\n",
       "      <td>B0038YJ4MU</td>\n",
       "      <td>A1AZ21Z4JQEQZU</td>\n",
       "      <td>JRTN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1340150400</td>\n",
       "      <td>Something that works!</td>\n",
       "      <td>As a chronic insomniac, I have tried most prod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId         ProfileName  \\\n",
       "442836  478823  B001QZYFOU   AEPG7I28BZKZB             Silcat3   \n",
       "226042  245088  B000GFYRHG  A2K3J2X8KDY47N  Jewelry Lover \"me\"   \n",
       "232542  252248  B0046GSTUM  A2RUR1SMPNGKXJ               Katie   \n",
       "516810  558761  B000BVY02M   AOEDWQLH2WKKW   E. J Tastad \"ejt\"   \n",
       "141203  153228  B0038YJ4MU  A1AZ21Z4JQEQZU                JRTN   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "442836                     0                       0  positive  1346803200   \n",
       "226042                     0                       0  positive  1306972800   \n",
       "232542                     2                       2  positive  1345507200   \n",
       "516810                    21                      21  positive  1169424000   \n",
       "141203                     0                       0  positive  1340150400   \n",
       "\n",
       "                                            Summary  \\\n",
       "442836                             Yum, but mash it   \n",
       "226042                 My all time favorite tea....   \n",
       "232542                     Excellent very delicious   \n",
       "516810  Hot sauce concentrate, use at your own risk   \n",
       "141203                        Something that works!   \n",
       "\n",
       "                                                     Text  \n",
       "442836  Our three love the meat, and lick the plate if...  \n",
       "226042  I am now receiving a case of this tea every si...  \n",
       "232542  I am on a strict diet and this is a very delic...  \n",
       "516810  This is like hot sauce concentrate.  You MUST ...  \n",
       "141203  As a chronic insomniac, I have tried most prod...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_final = final.sample(10000) #Sampling Randomly 10k points from 364k\n",
    "sample_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138683</th>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179643</th>\n",
       "      <td>194858</td>\n",
       "      <td>B0000E65WB</td>\n",
       "      <td>A2VZ11U5DXM8J5</td>\n",
       "      <td>C. Ebeling \"ctlpareader\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1068336000</td>\n",
       "      <td>Stock Up On This Item</td>\n",
       "      <td>I usually purchase this item in smaller links ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390522</th>\n",
       "      <td>422248</td>\n",
       "      <td>B0000D9N9A</td>\n",
       "      <td>A3LFT71N1YOQXN</td>\n",
       "      <td>Bell Mays</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>positive</td>\n",
       "      <td>1068422400</td>\n",
       "      <td>Hot Sizzling bubbly Raclette! !  Bubbly Bubbly...</td>\n",
       "      <td>Put in under a Reclette grill or just put it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472979</th>\n",
       "      <td>511508</td>\n",
       "      <td>B0000D94P1</td>\n",
       "      <td>A2801SG8XA9LNX</td>\n",
       "      <td>PACW</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>positive</td>\n",
       "      <td>1069113600</td>\n",
       "      <td>Tastes great for what it is</td>\n",
       "      <td>I have relied on these cake mixes for a few ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370384</th>\n",
       "      <td>400533</td>\n",
       "      <td>B0000V8HTU</td>\n",
       "      <td>A6M8KOVEPQ0BO</td>\n",
       "      <td>Cyn \"cynnergy\"</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>1073865600</td>\n",
       "      <td>This is the best coffee!</td>\n",
       "      <td>Hawaii Roasters is definitely the best coffee ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId               ProfileName  \\\n",
       "138683  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "179643  194858  B0000E65WB  A2VZ11U5DXM8J5  C. Ebeling \"ctlpareader\"   \n",
       "390522  422248  B0000D9N9A  A3LFT71N1YOQXN                 Bell Mays   \n",
       "472979  511508  B0000D94P1  A2801SG8XA9LNX                      PACW   \n",
       "370384  400533  B0000V8HTU   A6M8KOVEPQ0BO            Cyn \"cynnergy\"   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "138683                     2                       2  positive   940809600   \n",
       "179643                     1                       1  positive  1068336000   \n",
       "390522                    14                      16  positive  1068422400   \n",
       "472979                    14                      15  positive  1069113600   \n",
       "370384                     2                       3  positive  1073865600   \n",
       "\n",
       "                                                  Summary  \\\n",
       "138683  This whole series is great way to spend time w...   \n",
       "179643                              Stock Up On This Item   \n",
       "390522  Hot Sizzling bubbly Raclette! !  Bubbly Bubbly...   \n",
       "472979                        Tastes great for what it is   \n",
       "370384                           This is the best coffee!   \n",
       "\n",
       "                                                     Text  \n",
       "138683  I can remember seeing the show when it aired o...  \n",
       "179643  I usually purchase this item in smaller links ...  \n",
       "390522  Put in under a Reclette grill or just put it i...  \n",
       "472979  I have relied on these cake mixes for a few ye...  \n",
       "370384  Hawaii Roasters is definitely the best coffee ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_final = sample_final.sort_values('Time', axis=0, ascending=True) #Sorting Based upon Time stamp.\n",
    "sample_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    8478\n",
       "negative    1522\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many Positives and Negatives that are present in Scores Columns.\n",
    "sample_final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing---> Removing Stop Words,Upper Case to Lower Case Conversion,Stemming,Lemmatizatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138683</th>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179643</th>\n",
       "      <td>I usually purchase this item in smaller links ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390522</th>\n",
       "      <td>Put in under a Reclette grill or just put it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472979</th>\n",
       "      <td>I have relied on these cake mixes for a few ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370384</th>\n",
       "      <td>Hawaii Roasters is definitely the best coffee ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text\n",
       "138683  I can remember seeing the show when it aired o...\n",
       "179643  I usually purchase this item in smaller links ...\n",
       "390522  Put in under a Reclette grill or just put it i...\n",
       "472979  I have relied on these cake mixes for a few ye...\n",
       "370384  Hawaii Roasters is definitely the best coffee ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_text = sample_final['Text']\n",
    "final_text = pd.DataFrame(final_text)\n",
    "final_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def nlp_preprocessing(total_text, index, column):\n",
    "    if type(total_text) is not int:\n",
    "        string_1 = \"\"\n",
    "        #Replace special chars with space\n",
    "        total_text = re.sub('[^a-zA-Z0-9\\n]',' ',total_text)\n",
    "        #Replace Multiple spaces with single space.\n",
    "        total_text = re.sub('\\s+',' ',total_text)\n",
    "        #Convert all upper case words to lower case.\n",
    "        total_text = total_text.lower()\n",
    "        #Stop word Removal.\n",
    "        for word_1 in total_text.split():\n",
    "            #If the word is not stop word then retain it, otherwise remove it.\n",
    "            if not word_1 in stop_words:\n",
    "                string_1 += word_1 + \" \"\n",
    "        total_text = string_1\n",
    "        #Stemming using Snowball Stemmer.\n",
    "        string_2 = \"\"\n",
    "        for word_2 in total_text.split():\n",
    "            string_2 += snowball_stemmer.stem(word_2) + \" \"\n",
    "        total_text = string_2\n",
    "        #Lemmatizer\n",
    "        string_3 = \"\"\n",
    "        for word_3 in total_text.split():\n",
    "            string_3 += wordnet_lemmatizer.lemmatize(word_3) + \" \"\n",
    "        \n",
    "        final_text[column][index] = string_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time took for preprocessing the text : 17.9822186546936 seconds\n"
     ]
    }
   ],
   "source": [
    "#Text Preprocessing stage.\n",
    "import time\n",
    "start_time = time.clock()\n",
    "for index,row in final_text.iterrows():\n",
    "    nlp_preprocessing(row['Text'], index, 'Text')\n",
    "print('Time took for preprocessing the text :', time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing Data into Train and Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138683    rememb see show air televis year ago child sis...\n",
      "179643    usual purchas item smaller link 9 pound stash ...\n",
      "390522    put reclett grill put oven 5 10 minut serv boi...\n",
      "472979    reli cake mix year find handi tast fine sugar ...\n",
      "370384    hawaii roaster definit best coffe glad get ama...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y_true = sample_final['Score'].values\n",
    "#Split the data as train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(final_text['Text'], y_true, test_size=0.3, shuffle=False)\n",
    "#Split the X_1 and y_1 into train and Cross validate\n",
    "#X_train, X_cv, y_train, y_cv = train_test_split(X_1, y_1, test_size=0.2, shuffle=False)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words(Bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting Text (or) Paragraphs to Vectors\n",
    "#Why vectors?\n",
    "#I can do all the mathematical operations vectors using Linear Algebra.\n",
    "count_vect = CountVectorizer() #from scikit Learn\n",
    "train_bow = count_vect.fit_transform(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_bow = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 12057)\n",
      "(3000, 12057)\n"
     ]
    }
   ],
   "source": [
    "print(train_bow.get_shape())\n",
    "print(test_bow.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_bow.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_bow, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('test_bow.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_bow, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "train_tfidf = tfidf_vect.fit_transform(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 12057)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12057"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tfidf_vect.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['162',\n",
       " '1670',\n",
       " '1696',\n",
       " '16oz',\n",
       " '16th',\n",
       " '17',\n",
       " '170',\n",
       " '1708',\n",
       " '170mg',\n",
       " '175',\n",
       " '17lbs']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[109:120] #Printing 10 Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Convert a row in Sparse Matrix to numpy array\n",
    "print(train_tfidf[3,:].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a Pickle file.\n",
    "with open('train_tfidf.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('test_tfidf.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('y_train.pickle', 'wb') as handle:\n",
    "    pickle.dump(y_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('y_test.pickle', 'wb') as handle:\n",
    "    pickle.dump(y_test, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test split for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138683    I can remember seeing the show when it aired o...\n",
      "179643    I usually purchase this item in smaller links ...\n",
      "390522    Put in under a Reclette grill or just put it i...\n",
      "472979    I have relied on these cake mixes for a few ye...\n",
      "370384    Hawaii Roasters is definitely the best coffee ...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y_true = sample_final['Score'].values\n",
    "#Split the data as train and test \n",
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(sample_final['Text'], y_true, test_size=0.3, shuffle=False)\n",
    "#Split the X_1 and y_1 into train and Cross validate\n",
    "#X_train_w, X_cv_w, y_train_w, y_cv_w = train_test_split(X_1, y_1, test_size=0.2, shuffle=False)\n",
    "print(X_train_w.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanhtml(sentence): #function to clean the word of any html tags.\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr,' ',sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence): #Function to clean words of punctuation.\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|(|)|\\|/]',r'',cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from gensim.models import Word2Vec.\n",
    "#Train your own Word2Vec Model using our own text Corpus.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "\n",
    "def avg_word2vec(final_text):\n",
    "    i=0\n",
    "    list_of_sent=[]\n",
    "    for sent in final_text.values:\n",
    "        filtered_sentence=[]\n",
    "        sent=cleanhtml(sent)\n",
    "        for w in sent.split():\n",
    "            for cleaned_words in cleanpunc(w).split():\n",
    "                if (cleaned_words.isalpha()):\n",
    "                    filtered_sentence.append(cleaned_words.lower())\n",
    "                else:\n",
    "                    continue\n",
    "        list_of_sent.append(filtered_sentence)\n",
    "\n",
    "    #Word2Vec Model\n",
    "    w2v_model = gensim.models.Word2Vec(list_of_sent,min_count=5,size=300,workers=4)\n",
    "\n",
    "    #Avg word2vec is done only for 10k points\n",
    "    #Average Word2Vec\n",
    "    #Compute average w2v for each review\n",
    "    sent_vectors = []; \n",
    "    for sent in list_of_sent:\n",
    "        sent_vec = np.zeros(300)\n",
    "        cnt_words = 0;\n",
    "        for word in sent:\n",
    "            try:\n",
    "                vec = w2v_model.wv[word]\n",
    "                sent_vec += vec\n",
    "                cnt_words += 1\n",
    "            except:\n",
    "                pass\n",
    "        sent_vec /= cnt_words\n",
    "        sent_vectors.append(sent_vec)\n",
    "    print(len(sent_vectors))\n",
    "    print(len(sent_vectors[0]))\n",
    "    return sent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "train_avg_word2vec = avg_word2vec(X_train_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.13575224e-02, -4.47940966e-02,  6.17986763e-02, -4.98300192e-02,\n",
       "       -2.34878440e-01,  2.13162693e-02, -1.87575054e-01,  2.20469415e-02,\n",
       "        2.20825537e-02, -1.61932165e-01, -1.27512975e-04,  9.88196107e-02,\n",
       "        1.12091546e-01, -4.29454954e-02, -5.12704245e-02, -1.18187675e-01,\n",
       "        1.23256331e-02, -1.70489881e-01,  7.29508118e-02, -8.55551501e-02,\n",
       "       -3.40148298e-02,  1.30341168e-01, -4.36049535e-02,  6.85732573e-02,\n",
       "       -1.40729885e-01, -1.33843139e-01,  1.69613931e-02, -1.89379374e-01,\n",
       "        1.21075724e-01,  3.30583367e-02, -5.75963351e-02,  1.37715322e-02,\n",
       "        3.02028746e-03, -8.04775749e-02, -6.70864611e-02, -1.06477294e-01,\n",
       "       -2.11554136e-02, -2.08362241e-01, -4.02353493e-03, -3.81884192e-02,\n",
       "        2.15567998e-01,  5.15320052e-02, -6.99333848e-03, -1.07191285e-01,\n",
       "        1.67498763e-01, -8.68687229e-02, -1.33854655e-01,  1.33357496e-01,\n",
       "        6.26992199e-02, -1.73796219e-02,  1.81256981e-02, -3.76669619e-03,\n",
       "       -1.34030752e-01,  1.68191305e-01,  5.67499399e-02, -5.35313348e-02,\n",
       "       -5.47064338e-02,  8.64163537e-02,  1.75639039e-01, -5.50248449e-02,\n",
       "        1.58404738e-01,  8.03036365e-02,  1.99135146e-04,  1.56286455e-01,\n",
       "       -4.71905983e-03, -1.04428033e-01, -9.51708671e-02, -2.99704476e-02,\n",
       "        2.84251930e-01,  3.14656305e-01, -1.13414676e-01,  1.98417787e-01,\n",
       "       -3.44532609e-02,  6.49928495e-02, -9.66041628e-02, -1.24560021e-01,\n",
       "       -2.60193414e-01,  1.83934819e-01,  7.14312042e-02, -1.93013689e-02,\n",
       "        9.05399024e-02,  2.64305950e-02,  5.37037393e-02, -3.39544045e-02,\n",
       "       -2.31156891e-01,  3.98900810e-02,  2.02511274e-01, -1.10181214e-01,\n",
       "       -1.83236621e-05, -3.49188731e-01,  1.34935793e-01, -4.26942932e-02,\n",
       "        4.71061526e-02, -2.46999456e-01,  3.19117068e-01, -4.19786975e-02,\n",
       "       -2.14290685e-01,  5.09411792e-02, -2.19144013e-01,  3.51874193e-02,\n",
       "        4.72592490e-02,  5.00526313e-02,  6.52635380e-02,  1.72253181e-02,\n",
       "        6.33564324e-02,  6.84070493e-02,  2.77957089e-01, -3.29386143e-02,\n",
       "        3.29239844e-02, -2.00097685e-02,  4.08368431e-02, -3.31671554e-02,\n",
       "        9.16141341e-02,  1.53715856e-01, -7.98897187e-02, -4.98316045e-02,\n",
       "       -2.24867035e-01, -5.15450220e-02,  9.81261256e-02,  1.18835971e-01,\n",
       "       -5.79543347e-02, -5.12774394e-02,  6.28602968e-02,  1.43842324e-01,\n",
       "       -2.82721978e-01, -6.44324765e-02, -9.95395490e-02, -1.08519464e-02,\n",
       "        1.50484930e-01,  8.13617070e-02,  2.18968927e-01, -2.03799600e-02,\n",
       "       -1.81559138e-01, -1.17878023e-01, -3.70199950e-01,  3.41876185e-02,\n",
       "        4.79429059e-02,  7.14405655e-02,  4.76067213e-02, -2.02042806e-02,\n",
       "       -1.53477018e-01, -1.50247784e-02,  2.26620931e-01, -3.73068517e-02,\n",
       "       -3.43163130e-02,  8.82140276e-02,  6.51835739e-02,  1.80683365e-02,\n",
       "       -4.05357776e-03, -1.83694464e-01,  2.87628333e-02,  1.86338775e-01,\n",
       "       -6.70748827e-02, -1.15363090e-01,  1.76831116e-01, -1.04822512e-01,\n",
       "        1.50196256e-01, -4.14767624e-02,  1.96353997e-01,  8.40753464e-02,\n",
       "       -4.71826965e-02, -2.44738778e-01, -2.65707462e-01, -2.29973398e-01,\n",
       "       -7.69579264e-02,  1.53627385e-01, -9.41772231e-02,  1.85542670e-01,\n",
       "       -5.61401403e-02, -7.90655252e-02, -1.71360434e-01, -1.73292041e-01,\n",
       "        8.08961948e-02,  8.78272032e-02, -2.10463209e-01, -2.15691025e-02,\n",
       "        1.76545261e-01, -1.71650060e-02, -7.18977067e-02,  3.34194988e-02,\n",
       "        2.16374339e-02, -2.31278281e-01, -7.48429642e-02,  2.55399168e-01,\n",
       "       -1.53287793e-01,  2.89494137e-01,  2.91462218e-01,  5.38838668e-02,\n",
       "       -9.69991415e-02, -1.42201339e-01, -4.07285466e-02, -1.11979176e-01,\n",
       "        5.19404900e-04, -8.56302940e-02,  6.39173759e-02, -2.73745903e-02,\n",
       "        1.50000000e-03, -3.46430023e-02,  1.46927967e-01, -6.29144499e-02,\n",
       "       -9.31572772e-04, -3.46863969e-02, -3.37197117e-01,  1.31744549e-02,\n",
       "        5.76506800e-03,  1.12906612e-01, -2.44093282e-01, -2.49243843e-02,\n",
       "       -1.51866467e-01,  1.19592803e-01, -2.14993011e-01, -2.03525146e-01,\n",
       "       -2.72487001e-02,  3.19999717e-01, -1.47176698e-01, -1.16449823e-01,\n",
       "        5.32024133e-02, -1.62187359e-01, -1.13388358e-01, -3.92141895e-02,\n",
       "        1.31335562e-02,  1.23025914e-01,  8.30198934e-02,  1.15706302e-01,\n",
       "        1.30858789e-01, -2.57067195e-01, -1.25732450e-01, -2.01658952e-01,\n",
       "        1.17729823e-02, -3.76506414e-02,  2.15439978e-01,  2.49612525e-01,\n",
       "        1.21771516e-02, -1.58154751e-02, -1.26678808e-01, -3.53638123e-01,\n",
       "       -2.03883805e-03,  1.41101108e-01, -1.62763476e-02, -1.90676600e-01,\n",
       "        9.37177429e-02,  1.70405663e-02,  3.59214523e-01, -3.45774094e-02,\n",
       "        1.41455242e-01,  8.87485307e-02, -2.86743717e-02, -2.21627012e-01,\n",
       "       -1.90248298e-01, -2.47401164e-01, -1.59956715e-01, -1.84153416e-01,\n",
       "        8.46147844e-02, -1.24805149e-01,  7.36706731e-02,  8.77561826e-03,\n",
       "       -2.52428445e-01, -4.92597427e-02, -1.26870130e-01,  2.85325050e-01,\n",
       "       -1.42864905e-01, -5.16299295e-02,  2.31461334e-01, -3.60573248e-01,\n",
       "        8.35481335e-02,  5.14016341e-02,  9.62296156e-02,  7.36033807e-02,\n",
       "       -1.42826024e-03, -3.24166905e-01, -4.71231299e-02,  6.79758113e-02,\n",
       "       -1.43250254e-01,  1.11325079e-01,  9.55986893e-02, -2.31711332e-01,\n",
       "        4.49889782e-02, -2.01791612e-01, -1.42702025e-01, -1.99749026e-01,\n",
       "       -8.95909296e-02, -6.61235060e-03,  5.03671973e-02, -2.45281574e-03,\n",
       "       -1.12661000e-01,  1.60548271e-02,  3.96779027e-02,  2.95396307e-01,\n",
       "        3.64974052e-01,  1.57049235e-01, -8.73438038e-02,  2.42277863e-01,\n",
       "        9.28365560e-02, -6.54996365e-02, -1.21271527e-01, -1.86155116e-01,\n",
       "       -2.71297449e-01,  1.91939486e-01,  1.56582491e-01,  3.01494032e-02])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_avg_word2vec[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "test_avg_word2vec = avg_word2vec(X_test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a Pickle file.\n",
    "with open('train_avg_word2vec.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_avg_word2vec, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('test_avg_word2vec.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_avg_word2vec, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Weighted Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_word2vec(final_text):\n",
    "    i=0\n",
    "    list_of_sent=[]\n",
    "    for sent in final_text.values:\n",
    "        filtered_sentence=[]\n",
    "        sent=cleanhtml(sent)\n",
    "        for w in sent.split():\n",
    "            for cleaned_words in cleanpunc(w).split():\n",
    "                if (cleaned_words.isalpha()):\n",
    "                    filtered_sentence.append(cleaned_words.lower())\n",
    "                else:\n",
    "                    continue\n",
    "        list_of_sent.append(filtered_sentence)\n",
    "\n",
    "    #Word2Vec Model\n",
    "    w2v_model = gensim.models.Word2Vec(list_of_sent,min_count=5,size=300,workers=4)\n",
    "\n",
    "    #Tfidf weigted word2vec\n",
    "    tfidf_feat = tfidf_vect.get_feature_names() #tfidf words/col-names\n",
    "    tfidf_sent_vectors = [];\n",
    "    row = 0;\n",
    "    for sent in list_of_sent:\n",
    "        sent_vec = np.zeros(300)\n",
    "        weight_sum = 0;\n",
    "        for word in sent:\n",
    "            try:\n",
    "                vec = w2v_model.wv[word]\n",
    "                tf_idf = final_tf_idf[row,tfidf_feat.index(word)]\n",
    "                sent_vec += (vec * tf_idf)\n",
    "                weight_sum += tf_idf\n",
    "            except:\n",
    "                pass\n",
    "        sent_vec /= weight_sum\n",
    "        tfidf_sent_vectors.append(sent_vec)\n",
    "        row += 1\n",
    "\n",
    "    print(tfidf_sent_vectors[2])\n",
    "    return tfidf_sent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "train_tfidf_word2vec = avg_word2vec(X_train_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13468803,  0.04505837,  0.05099325,  0.03937044, -0.42393803,\n",
       "       -0.02780443, -0.26548989, -0.00140673, -0.11699958, -0.19363701,\n",
       "       -0.01939635,  0.15379193,  0.05386574,  0.1139471 , -0.05895299,\n",
       "       -0.10969904,  0.15326163, -0.07685471,  0.0461257 ,  0.00404729,\n",
       "       -0.00212115,  0.23534476,  0.13289246,  0.02117112, -0.09087324,\n",
       "       -0.12764124,  0.03217189, -0.26916636,  0.04226346, -0.0551585 ,\n",
       "       -0.06161767,  0.04566315, -0.04511133,  0.02185931, -0.10457619,\n",
       "       -0.11435286, -0.04377859, -0.1411576 ,  0.1483881 ,  0.16589516,\n",
       "        0.08295923, -0.0249235 ,  0.02431173, -0.0440818 ,  0.16415046,\n",
       "        0.00066396, -0.02563621,  0.11759276,  0.1663628 , -0.01327317,\n",
       "       -0.09856221,  0.08839298, -0.24811286,  0.25382858, -0.04463097,\n",
       "        0.16833731, -0.14997524,  0.05852601,  0.15804391, -0.03989062,\n",
       "        0.13692023,  0.06251453,  0.08666201,  0.1835815 ,  0.09227506,\n",
       "       -0.07693344, -0.12836327, -0.08873512,  0.20354578,  0.2733882 ,\n",
       "       -0.03661797,  0.23095546,  0.00355877,  0.02601149, -0.11483221,\n",
       "       -0.12142156, -0.32096974,  0.08300678,  0.11044459,  0.00313643,\n",
       "       -0.00678273, -0.06668523,  0.11453148, -0.07162491, -0.23943841,\n",
       "        0.05458899,  0.11898973, -0.06544532,  0.02082333, -0.24200736,\n",
       "        0.10155944,  0.01788039,  0.07678773, -0.21292691,  0.20195782,\n",
       "        0.03249564, -0.01871003,  0.03462807, -0.14942388,  0.08947022,\n",
       "        0.00648162,  0.15647186,  0.08318666, -0.06387964,  0.06304886,\n",
       "        0.11705006,  0.26649197,  0.02642624, -0.00069351, -0.13155767,\n",
       "        0.04978064,  0.01182272,  0.15377195,  0.14121701,  0.01351551,\n",
       "       -0.1825458 , -0.13918662, -0.14875843,  0.14041267,  0.02304809,\n",
       "       -0.03787783, -0.07162457, -0.06112843,  0.14405136, -0.32790709,\n",
       "       -0.10884868, -0.06707474, -0.04286431,  0.21647521,  0.05550026,\n",
       "        0.13604138,  0.01952031, -0.12477406, -0.16737275, -0.30184547,\n",
       "        0.03015228,  0.0755834 ,  0.08715814,  0.02843542, -0.07990561,\n",
       "       -0.2687358 ,  0.02416548,  0.21243032,  0.08930014, -0.10004408,\n",
       "        0.32349648,  0.13201046,  0.01822777,  0.09578207, -0.18156139,\n",
       "        0.04313311,  0.30482141,  0.02786214, -0.06041578,  0.15846241,\n",
       "       -0.06866186,  0.09426429, -0.06874777,  0.21822454,  0.08732146,\n",
       "       -0.04810669, -0.29293824, -0.1810472 , -0.3753535 , -0.10546247,\n",
       "        0.08791771,  0.02136431, -0.05654054, -0.03668653, -0.06821626,\n",
       "       -0.19595566, -0.09439771,  0.11801139,  0.03985996, -0.233222  ,\n",
       "        0.01801196,  0.16465788,  0.09161207, -0.01440945, -0.1072672 ,\n",
       "       -0.00303869, -0.17571656,  0.0300111 ,  0.229797  , -0.03684362,\n",
       "        0.25867408,  0.20090968,  0.06833953, -0.06951869, -0.05634505,\n",
       "       -0.05993392, -0.11385212, -0.10814397,  0.00230776,  0.16345478,\n",
       "        0.07718855, -0.03193522, -0.07214821,  0.07324543, -0.17249656,\n",
       "        0.00243971, -0.11099495, -0.26719342,  0.0739383 ,  0.06250421,\n",
       "        0.19623929, -0.29322028,  0.0316834 , -0.0419422 ,  0.10909639,\n",
       "       -0.12372027, -0.29990394,  0.00745074,  0.37683907, -0.13128256,\n",
       "       -0.173488  ,  0.10068726, -0.11049562, -0.04131547, -0.104065  ,\n",
       "       -0.00407994,  0.13911943,  0.02381541,  0.12857267, -0.00721297,\n",
       "       -0.12074231,  0.0167501 , -0.14719476,  0.00369088,  0.0866222 ,\n",
       "        0.18949868,  0.23530329, -0.05534013, -0.05832227, -0.12771724,\n",
       "       -0.22534515, -0.14554625,  0.03016336,  0.07215433, -0.18800659,\n",
       "        0.07646512,  0.10429492,  0.19241112,  0.00117001,  0.08047549,\n",
       "       -0.02972826, -0.06792308, -0.17655558, -0.13616369, -0.11690463,\n",
       "       -0.26631897, -0.1393518 ,  0.08282167, -0.14347895,  0.10656273,\n",
       "       -0.03795165, -0.19810591, -0.09468105, -0.15792566,  0.21990643,\n",
       "       -0.02417694, -0.02139359,  0.1141313 , -0.34777468,  0.03729174,\n",
       "        0.1082845 ,  0.0313774 ,  0.08314933, -0.07088324, -0.26863211,\n",
       "       -0.06335691,  0.05474725, -0.12387085,  0.04061597,  0.09167525,\n",
       "       -0.11645869,  0.011291  , -0.09562417, -0.15800775, -0.20561276,\n",
       "       -0.09109286,  0.0985308 , -0.01698206, -0.00899184,  0.02534906,\n",
       "       -0.02412548,  0.00458151,  0.2946573 ,  0.24481286,  0.11132566,\n",
       "       -0.09297459,  0.28849727,  0.01412068, -0.04320848, -0.06468672,\n",
       "       -0.09868742, -0.15689022,  0.10722072,  0.02132915,  0.06684071])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf_word2vec[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "test_tfidf_word2vec = avg_word2vec(X_test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Pickle file.\n",
    "with open('train_tfidf_word2vec.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_tfidf_word2vec, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('test_tfidf_word2vec.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_tfidf_word2vec, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('y_train_w.pickle', 'wb') as handle:\n",
    "    pickle.dump(y_train_w, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('y_test_w.pickle', 'wb') as handle:\n",
    "    pickle.dump(y_test_w, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
