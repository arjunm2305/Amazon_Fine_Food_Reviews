{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting DT on Amazon Food Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSet Source: https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arjun\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importing Python Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loding Bow data:\n",
    "import pickle\n",
    "with open('train_bow.pickle','rb') as handle:\n",
    "    train_bow = pickle.load(handle)\n",
    "with open('test_bow.pickle','rb') as handle:\n",
    "    test_bow = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loding Tfidf data:\n",
    "import pickle\n",
    "with open('train_tfidf.pickle','rb') as handle:\n",
    "    train_tfidf = pickle.load(handle)\n",
    "with open('test_tfidf.pickle','rb') as handle:\n",
    "    test_tfidf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loding Avg Word2Vec data:\n",
    "import pickle\n",
    "with open('train_avg_word2vec.pickle','rb') as handle:\n",
    "    train_avg_word2vec = pickle.load(handle)\n",
    "with open('test_avg_word2vec.pickle','rb') as handle:\n",
    "    test_avg_word2vec = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loding Avg Word2Vec data:\n",
    "import pickle\n",
    "with open('train_tfidf_word2vec.pickle','rb') as handle:\n",
    "    train_tfidf_word2vec = pickle.load(handle)\n",
    "with open('test_tfidf_word2vec.pickle','rb') as handle:\n",
    "    test_tfidf_word2vec = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('y_train.pickle','rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "with open('y_test.pickle','rb') as handle:\n",
    "    y_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('y_train_w.pickle','rb') as handle:\n",
    "    y_train_w = pickle.load(handle)\n",
    "with open('y_test_w.pickle','rb') as handle:\n",
    "    y_test_w = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature Normalization\n",
    "from sklearn.preprocessing import normalize\n",
    "#Bow features\n",
    "train_bow_normalize = normalize(train_bow, axis=0)\n",
    "test_bow_normalize = normalize(test_bow, axis=0)\n",
    "#Tfidf Features\n",
    "train_tfidf_normalize = normalize(train_tfidf, axis=0)\n",
    "test_tfidf_normalize = normalize(test_tfidf, axis=0)\n",
    "#Avg word2Vec Features\n",
    "train_avgw2v_normalize = normalize(train_avg_word2vec, axis=0)\n",
    "test_avgw2v_normalize = normalize(test_avg_word2vec, axis=0)\n",
    "#Tfidf Weighted Word2Vec\n",
    "train_tfidfw2v_normalize = normalize(train_tfidf_word2vec, axis=0)\n",
    "test_tfidfw2v_normalize = normalize(test_tfidf_word2vec, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization: Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=300,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "0.8706666666666667\n"
     ]
    }
   ],
   "source": [
    "#GridSearch to find No of BaseLearners, depth and LearningRate.\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#n_estimators = [100, 200, 300, 400, 500]\n",
    "#learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "#param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "param_grid = {'n_estimators':[100, 300, 500, 800], 'max_depth':[3, 5, 10]}\n",
    "model = GridSearchCV(GradientBoostingClassifier(), param_grid, scoring='accuracy')\n",
    "model.fit(train_bow_normalize, y_train)\n",
    "print(model.best_estimator_)\n",
    "print(model.score(test_bow_normalize,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization: Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=5,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=300,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "0.8703333333333333\n"
     ]
    }
   ],
   "source": [
    "#GridSearch to find No of BaseLearners, depth and LearningRate.\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#n_estimators = [100, 200, 300, 400, 500]\n",
    "#learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "#param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "param_grid = {'n_estimators':[100, 300, 500, 800], 'max_depth':[3, 5, 10]}\n",
    "model = GridSearchCV(GradientBoostingClassifier(), param_grid, scoring='accuracy')\n",
    "model.fit(train_tfidf_normalize, y_train)\n",
    "print(model.best_estimator_)\n",
    "print(model.score(test_tfidf_normalize,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization: Avg Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=5,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=800,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "0.795\n"
     ]
    }
   ],
   "source": [
    "#GridSearch to find No of BaseLearners, depth and LearningRate.\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#n_estimators = [100, 200, 300, 400, 500]\n",
    "#learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "#param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "param_grid = {'n_estimators':[100, 300, 500, 800], 'max_depth':[3, 5, 10]}\n",
    "model = GridSearchCV(GradientBoostingClassifier(), param_grid, scoring='accuracy')\n",
    "model.fit(train_avgw2v_normalize, y_train_w)\n",
    "print(model.best_estimator_)\n",
    "print(model.score(test_avgw2v_normalize,y_test_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization: Tfidf Weighted Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "0.823\n"
     ]
    }
   ],
   "source": [
    "#GridSearch to find No of BaseLearners, depth and LearningRate.\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#n_estimators = [100, 200, 300, 400, 500]\n",
    "#learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "#param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "param_grid = {'n_estimators':[100, 300, 500, 800], 'max_depth':[3, 5, 10]}\n",
    "model = GridSearchCV(GradientBoostingClassifier(), param_grid, scoring='accuracy')\n",
    "model.fit(train_tfidfw2v_normalize, y_train_w)\n",
    "print(model.best_estimator_)\n",
    "print(model.score(test_tfidfw2v_normalize, y_test_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Observation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are getting better performance with bag of words featurization with n_estimators=300 and max_depth=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
